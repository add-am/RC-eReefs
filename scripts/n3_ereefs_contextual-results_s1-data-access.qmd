---
title: "eReefs Contextual Results - Data Access"
subtitle: "A Healthy Waters Partnership Analysis"
description: "This script has been written to create a series of contextual results to be included in the Offshore Marine Zone of the Technical Report."
author: "Adam Shand" 
format: html
params:
  project_crs: "EPSG:7844"
  target_fyear: 2023
  target_region: "Dry Tropics"
  resolution: 0 #0 is full size, bigger number = more cells merge together
  buffer: 1.2 #how much "extra" of the model do we want to get outside of the focus area
---

::: {.callout-note}
This is one part of several scripts exploring CSIRO ereefs data. 
:::

# Introduction

The only objective of this script is to download data from the eReefs platform. For detailed explanations of this process, refer to earlier scripts such as ereefs script 6 (found in the archives).

This script will take several inputs:

 - Region(s)
 - Financial Year(s)
 - Variable(s)
 - Downsample
 - Buffer

The user must supply **at least one of each**, however could choosen to provide several. The following cheatsheet has been supplied for common inputs:

| Input      | Full Names/Context                                                        | Text to Supply To Script                                  |
| ---------- | ------------------------------------------------------------------------- | --------------------------------------------------------- |
| Region     | Dry Tropics, Wet Tropics, Burdekin, Mackay Whitsunday Issac               | DT, WT, BD, MWI                                           |
| FYear      | 2019-20, 2020-21, 2021-22, 2022-23, 2023-24                               | 2020, 2021, 2022, 2023, 2024                              |
| Variable   | Turbidity, Chlorophyll a, DIN, Ammonium, Nitrate, Secchi Depth, pH, Wind  | Turbidity, Chlorophyll a, DIN, NH4, NO3, Secchi, pH, Wind |
| Downsample | 0-10, integer (no decimal), increasing by 1 each time                     | 0 - 10*                                                   |
| Buffer     | Yes, No                                                                   | Y, N**                                                    |

* Downsample indicates the amount cell aggregation, with 0 being 0 cells aggregated and 10 being 10 cells in each direction combined into a single cell.
** Buffer indicates the additional extent outside the region to the edge of the maps, Yes means include this extra area, No means do not.

:::{.callout-warning}
The more region, financial year, and variable inputs supplied, the longer the script will take to process. To improve processing the data will be saved to your local computer and reloaded next time (approximate 30 seconds). 
:::

:::{.callout-note}
The selection of "years" is mildly confusing as file dates are based on when loggers where deployed/retrieved in the field - and are not based on any logical "cut-off" day. Thus some files may extend across two years. Files are selected based on their **start** date, if the deployment started on the 30th of December 2023 and went until April 2024, the deployment is considered to be 2023. Below is a simple reference figure:

![](/references/images/n3_ereefs_contextual_results_s1-data-access/logger_deployments_over_years.png)
:::

# Script Set Up

This script requires multiple core spatial packages, each of which is loaded below. Key variables and paths are also created.

```{r}
#| label: load packages

#if this packages need to be installed:
#remotes::install_github("open-aims/ereefs")

#use pacman function to load and install (if required) all other packages
pacman::p_load(here, purrr, tidyverse, stringr, sf, glue, ereefs, stars, ncmeta)

#build data save path manually (it is just easier in this case)
#data_path <- here("data/n3_ereefs_contextual-results/")

#turn off spherical geometry
sf_use_s2(FALSE)

```

# Verify User Inputs

The first step is to verify all of the requested user inputs are valid and supported by the script:

```{r}

#manually enter user inputs to begin with for testing:
u_in <- list(
  u_in_reg = c("DT"),
  u_in_year = c(2020),#, 2021, 2022, 2023, 2024),
  u_in_var = c("Turbidity"), #"Chlorophyll a", "DIN", 
  #"Wind"),
  u_in_ds <- 6,
  u_in_bf <- "N")

#name each item in the list
names(u_in) <- c("Region", "FYear", "Variable", "Downsample", "Buffer")

#build vectors of supported inputs
u_in_supported <- list(
  reg_supported = c("DT", "WT", "BD", "MWI"),
  fyear_supported = c(2020, 2021, 2022, 2023, 2024),
  var_supported = c("Turbidity", "Chlorophyll a", "DIN", "NH4", "NO3", "Secchi", "pH", "Wind"),
  ds_supported = seq(0,10,1),
  bf_supported = c("Y", "N"))

#name each item in the list
names(u_in_supported) <- c("Region", "FYear", "Variable", "Downsample", "Buffer")

#for each item in the two lists, compare and recored differences
compare_inputs <- map2(u_in, u_in_supported, setdiff)

#remove any instances where there are no differences
compare_inputs <- flatten(compare_inputs)

if (length(compare_inputs) > 0){
  for (i in 1:length(compare_inputs)){

    wrong_input <- compare_inputs[[i]]

    input_type <- str_to_lower(names(compare_inputs[i]))

    input_options <- paste(u_in_supported[[names(compare_inputs[i])]], collapse = ", ")

    stop(glue('The "{wrong_input}" {input_type} input is not currently supported, try one of: "{input_options}"'))
  }
} else {print("All user inputs supported.")}

```

# Retrieve Data

Once inputs are verified eReefs data is retrived via the OpeNDAP protocol from the THREDDS servers. For more information, refer to previous scripts.

## Build Region Dataset

First we will need to define the region boundaries:

```{r}

#load in the n3 region
n3_region <- st_read(here("data/n3_region.gpkg"))

#refine the object to make the next step easier
n3_region <- n3_region |> 
  filter(Environment == "Marine") |> 
  group_by(Region, SubBasinOrSubZone) |> 
  summarise(geometry = st_union(geom)) |> 
  ungroup() |> st_cast() |> 
  st_make_valid()

#create a list of regions based on the users input
regions_selected_by_user <- map(u_in[["Region"]], function(x){

  #translate User Input
  region_key <- c(
    "DT" = "Dry Tropics",
    "WT" = "Wet Tropics",
    "BD" = "Burdekin",
    "MWI" = "Mackay Whitsunday Isaac"
  )
  
  #filter the data
  selected_region <- n3_region |> 
    filter(Region %in% unname(region_key[x]))

  #return the df as the final output (these will be wrapped up in a list)
  return(selected_region)
})

#add names to each item in the list to help track
names(regions_selected_by_user) <- u_in[["Region"]]

```

## Build Year Dataset

Next define the years of data the user wants (plus their associated layer index):

```{r}

#set up our indices reference table that converts (financial) year into the min and max layer index
year_key <- data.frame(
  "Fyear" = c(2020, 2021, 2022, 2023, 2024),
  "Start" = c(1, 216, 581, 946, 1311),
  "Count" = c(215, 365, 365, 365, 200)
  )

#pick the correct year(s)
target_layer_indices <- year_key |>
  filter(Fyear %in% u_in[["FYear"]])

```

## Expand Variable Inputs

If the user has requested "wind" this relates to four variables:

```{r}

#if wind requested
if (any(str_detect(u_in[["Variable"]], "Wind"))){

  #update the variable inputs
  u_in[["Variable"]] <- c(
    str_replace(u_in[["Variable"]], "Wind", "Wind Direction"),
    "Wind Stress",
    "Wind U Component",
    "Wind V Component")
}

```

## Build an Extraction Dataframe

Due to the variable number of inputs that can be created we will need to create a dataframe that contains each of the possible combinations requested by the user. This is achieved using the `expand.grid()` function. This df could range from 1 row to 27 rows depending on the number of inputs requested:

```{r}

#create a table that tracks all user inputs. This is used to provide inputs later on
all_user_inputs <- expand.grid(u_in, stringsAsFactors = F)

```

## Extract eReefs Data

The following code will use the extraction dataframe to check if the data is saved locally, if it is not local it will download it and save it locally.

```{r}

#using the same user inputs df
pmap(all_user_inputs, \(Region, FYear, Variable, Downsample, Buffer) {

  #translate the human readable variable names into the machine readable variable names (this is not done earlier as we need both versions for this function)
  var_rename <- c(
    "Turbidity" = "Turbidity", 
    "Chlorophyll a" = "Chl_a_sum", 
    "DIN" = "DIN", 
    "NH4" = "NH4", 
    "NO3" = "NO3", 
    "Secchi" = "Secchi", 
    "pH" = "PH", 
    "Wind Direction" = "wind_dir", 
    "Wind Stress" = "wind_mag",
    "Wind U Component" = "wind_u",
    "Wind V Component" = "wind_v"
  )

  #create a new vector of these machine readable names
  Variable_Machine <- var_rename[[Variable]]

  #create a filename using the human readable variable names
  file_name <- str_to_lower(
    str_replace_all(
      string = glue("{Region}_{FYear}_{Variable}_ds-{Downsample}_bf-{Buffer}"),
      pattern = " ",
      replacement = "-"
    )
  )

  #search for the file, if it exists locally, do nothing, otherwise download and save the data locally. Note MWI files are saved as RData
  if (any(file.exists(glue("{data_path}/{file_name}.{c('nc', 'RData')}")))){
    
  } else {

    #if the user wants buffer, set the value to 1.2, else set to 0
    if(Buffer == "Y"){ buf_val <- 1.2 } else { buf_val <- 0 }

    #get a bounding box of the marine region, add a buffer.
    region_bbox <- st_bbox(st_buffer(regions_selected_by_user[[Region]], buf_val))
      
    #rearrange to suit how eReefs like to request each point (order is min lon, max lon, min lat, max lat)
    region_bbox <- c(region_bbox[1], region_bbox[3], region_bbox[2], region_bbox[4])
      
    #establish the initial file path using the ereefs package (currently we use the eReefs GBR1 biogeochemistry and sediments v3.2 model)
    #input_file <- substitute_filename("catalog") #use this to see what options are available
    input_file <- "https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml"
        
    #get all grids
    grids <- get_ereefs_grids(input_file)
        
    #get x and y specifically
    x_grid <- grids[["x_grid"]]
    y_grid <- grids[["y_grid"]]
      
    #if the value is inside the bounds of each of our coords, change it to TRUE. Those outside are automatically false
    true_false_array <- x_grid >= region_bbox[1] & 
      x_grid <= region_bbox[2] & 
      y_grid >= region_bbox[3] & 
      y_grid <= region_bbox[4]
      
    #if the value is NA, change it to false.
    true_false_array[is.na(true_false_array)] <- FALSE

    #return the row index for every row that contains at least one true value:
    true_rows <- which(apply(true_false_array, 1, any))

    #find the first row that contains a true value
    first_row <- true_rows[1]

    #find the number of rows that contains a true value
    num_of_rows <- tail(true_rows, n = 1) - first_row

    #return the row index for every row that contains at least one true value:
    true_cols <- which(apply(true_false_array, 2, any))

    #find the first col that contains a true value
    first_col <- true_cols[1]

    #find the number of cols that contains a true value
    num_of_cols <- tail(true_cols, n = 1) - first_col

    #extract the layer indices associated with the year that has been targeted
    layer_indices_df <- filter(target_layer_indices, Fyear == FYear)

    #if the variable is secchi, or part of wind, the dimensions of the data are different (no depth)
    if (str_detect(Variable_Machine, "Secchi|wind")){
      
      #extract data using indices to define layer counts
      nc_data <- read_ncdf(
        input_file, 
        var = Variable_Machine,
        downsample = Downsample,
        ncsub = cbind(
          start = c(first_row, first_col, layer_indices_df[["Start"]]), 
          count = c(num_of_rows, num_of_cols, layer_indices_df[["Count"]]))
        )
    
    } else {

      #extract data using indices to define layer counts
      nc_data <- read_ncdf(
        input_file, 
        var = Variable_Machine,
        downsample = Downsample,
        ncsub = cbind(
          start = c(first_row, first_col, 44, layer_indices_df[["Start"]]), 
          count = c(num_of_rows, num_of_cols, 1, layer_indices_df[["Count"]]))
        )
    }
    
    #overwrite erroneous high values (note that a value of even 50 would be very very high)
    nc_data[(nc_data > 2000)] <- NA

    #extract units from the input
    data_unit <- filter(nc_atts(input_file, Variable_Machine), name == "units")$value[[1]]

    #add units to the data, they are not carried over well in stars objects so we will hide them in the attribute name
    names(nc_data) <- glue("{Variable} ({data_unit})")

    #if the region is MWI, then we can't convert data from curvilinear to rectilinear for a bunch of reasons... so we just dont!
    if (Region == "MWI"){

      #drop the depth dimension as this only has one layer
      nc_data <- nc_data[drop = TRUE]

      #save the data as an R object (note, this is much less efficent)
      saveRDS(nc_data, file = glue("{data_path}/{file_name}.RData"))

      #save the data locally as an R object rather than a 
      #write_mdim(x = nc_data, filename = glue("{data_path}/{file_name}_copy.nc"))

    } else {
      
      #update the crs on the data (move from lat long to meters)
      nc_data <- nc_data |> st_transform("EPSG:7855")

      #convert our curvilinear object into just a bbox
      curvilinear_bbox <- nc_data |> 
        st_bbox() |>
        st_as_sfc()

      #get a linear grid target with the same dimensions (number of cells) as our curvilinear grid 
      reg_stars <- st_as_stars(
        curvilinear_bbox, #using the bbox to provide the xmin, xmax etc., 
        nx = dim(nc_data)[[1]], #and the dimensions to provide the x and y count. 
        ny = dim(nc_data)[[2]], 
        values = NA_real_) #Fill each cell with NA

      #run st warp, it requires a curvilinear object, and a regular object as a target
      warped_data <- st_warp(nc_data, reg_stars)

      #drop the depth dimension as this only has one layer
      final_data <- warped_data[drop = TRUE]

      #save the data locally
      write_mdim(x = final_data, filename = glue("{data_path}/{file_name}.nc"))
    }
  }
})

```

This concludes the data access script.

```{r}

#just to access the "run above" button2


```